REINFORCE is a policy gradient method which directly learns a stochastic policy $\pi_\theta(a|s)$. Discounted returns $G_t$ are caluclated over a full episode of simulation, parameters $\theta$ of the policy model are updated such that the log probability of each action $a_t$ given state $s_t$ will increase or decrease in proportion to the return $G_t$. Parameters are updated with learning rate $\alpha$, by: 

$\theta \leftarrow  \theta + \alpha G_t \nabla_\theta \log (\pi_\theta(a_t|s_t))$

A linear model was tested, and though it was possible to simulate the provided heuristic model through weight initialisation, improvement was limited. 
A simple neural network with a single hidden layer (32 units), and two different output architectures tested. The first using discrete meta-actions (up, down, left, right, hover). Training was fraught with gradient instability with frequenct policy collapse, typically to an “action X is always best” model. Though weight decay and gradient clipping improved stability, this approach was discarded. 
The second and prefered architecture used continuous outputs modelling Gaussian distributions over thrust and roll, which were then mapped deterministically to left and right thrusts. This formulation was hypothesised more appropriate for the kinematic state space, removing the need for the model to learn thrust coupling. Standard deviations were fixed during initial experiments during hyperparameter and reward analyses, though these were eventually incorporated into the model as adaptive exploration parameters.

REINFORCE is notorious for instability as it is subject to high variance, and this was observed in initial experiments. To overcome this, a number of simple normalisation tricks were implemented, including state space normalisation, Xavier weight initialisation, and batch normalisation of returns. A batch mean baseline was used to overcome assymetric distribution in returns, important, for example, if all returns are net negative then even good actions result in negative gradients and are therefore penalised.
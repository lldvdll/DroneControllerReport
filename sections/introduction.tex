\section{Introduction}
Underactuated systems, characterised by fewer control inputs than degrees of freedom, present fundamental challenges in autonomous navigation [1]. We study a planar drone with three degrees of freedom ($x, y, \theta$) controlled by two upward thrusts $(T_1, T_2)$. Since the drone must tilt to move laterally, actions exhibit delayed kinematic effects which classical physics-based controllers often struggle to compensate for. To address this, we employ Reinforcement Learning (RL) to derive anticipatory policies capable of momentum management and collision avoidance for sequential target acquisition within hard boundaries.
We compare two RL approaches. \textbf{REINFORCE}, a policy gradient method, optimises a shallow neural network in continuous space. \textbf{SARSA}, an on-policy value-based method, employs a tabular approach with state discretisation and heuristic macro-actions. We demonstrate that while \textbf{REINFORCE} achieves only transient stability without successful navigation, \textbf{SARSA} converges to robust boundary-aware policies that outperform a heuristic controller in target acquisition by xx% and path efficiency by xx%.
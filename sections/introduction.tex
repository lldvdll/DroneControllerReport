\section{Introduction}
We study the autonomous navigation of an underactuated system, planar drone ($x, y, \theta$) controlled by two upward thrusts $(T_1, T_2)$. Since lateral motion requires prior rotation, actions exhibit delayed kinematic effects that physics-based controllers often struggle to manage. To address this, we employ Reinforcement Learning (RL) to derive anticipatory policies capable of momentum management and collision avoidance for sequential target acquisition within hard boundaries. We compare two RL approaches. \textbf{REINFORCE}, a continuous policy gradient method using a neural network, and \textbf{SARSA}, a discrete value-based method using tabular state-action values. We demonstrate that while \textbf{REINFORCE} achieves only temporary hover stability, \textbf{SARSA} converges to robust boundary-aware policies that outperform a heuristic controller in target acquisition by 28\% and path efficiency by 24\%.
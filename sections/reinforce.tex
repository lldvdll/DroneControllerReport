\section{REINFORCE}

% \subsection{Algorithm and Architecture}
We implement REINFORCE, a policy gradient method that directly optimises a stochastic policy $\pi_{\theta}(a|s)$. The parameters $\theta$ are updated via gradient ascent on the expected return $J(\theta)$, approximated by sampling trajectories:
\begin{equation}
    \theta \leftarrow \theta + \alpha G_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
\end{equation}
where $\alpha$ is the learning rate and $G_t$ is the discounted return, calculated over a full episode of simulation.

We use a neural network for the policy, with one hidden layer (32 units, ReLU) and Gaussian outputs ($\mu, \sigma$) for thrust and roll, mapped via sigmoid and tanh respectively to enforce kinematic constraints. This continuous formulation outperforms discrete baselines by learning adaptive exploration $\sigma$.

\medskip
REINFORCE is inherently high-variance, so we employed three stabilisation techniques. Input Normalisation to $(0,1)$ prevents feature bias. Batch stabilisation of discounted returns $G_t$ ($\mu=0, \sigma=1$) addressing asymmetric reward distributions (e.g., with all-negative returns, even good actions are penalised). Xavier weight initialisation to stabilise gradients.

\smallskip
We begin with a hover curriculum, where one permanent target is created at (0,0). Rewards are shaped to encourage target proximity and penalise instability. Coupled with a low discount factor (0.5), we aim to localise returns and to reward intention rather than outcome to overcome the return smoothing of REINFORCE. We note that Actor-Critic (AC) is a more principled approach to the same objective and should be investigated in future work.

Introducing a positive radius to a distance-to-target reward creates a basin of attraction around the target.
$r_{dist} = 0.25 - \Vert{\vec{x}_{target}} - {\vec{x}_{agent}}\Vert^2$. We penalise actions which amplify drift. Asymmetry is introduced to avoid catastrophic oscillation from overcorrection. $r_{drift} = min(0,-v_xroll)$

Training diagnostic plots (\Cref{fig:hover_success}) show monotonic increase in both cumulative rewards and survival time are correlated with gradual exploration decay and decrease in gradient variance. As the network settles into a consistent parameterisation, thrust and roll outputs converge with decreasing variance to counteract gravity. When tested, the agent converges slowly on targets, acquiring them with long enough episode length. The agent learned to stabilise near targets.

Though the intended second curriculum phase to learn navigation was thoroughly explored, the agent did not learn to navigate consistently to more than one target. Weight decay was introduced to escape the hover extrema and a value baseline was introduced using a target seeking SARSA model, howwever the agent showed little improvement in target seeking. Training diagnostics showed oscillating cumulative rewards, no significant improvement in policy entropy, and minimal increase in action variance. The agent learned stable hover behaviour, but learning to navigate from here would require either significant gradual unlearning, or better variance management.

\begin{figure}
    \centering
    \includegraphics[width=\singlefigure]{figures/neural_reinforce_hover_success.png}
    \caption{\label{fig:hover_success} Training diagnostics for hover policy. Top left: cumulative reward over training episodes (blue) plotted over survival time. Top right: mean and standard deviation of sampled actions. Bottom left: policy entropy as sigma averages representing exploration rate. Bottom right: gradient norms.}
\end{figure}

\smallskip
\section{REINFORCE}

\subsection{Algorithm and Architecture}
We implement REINFORCE, a policy gradient method that directly optimises a stochastic policy $\pi_{\theta}(a|s)$. The parameters $\theta$ are updated via gradient ascent on the expected return $J(\theta)$, approximated by sampling trajectories:
\begin{equation}
    \theta \leftarrow \theta + \alpha G_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
\end{equation}
where $\alpha$ is the learning rate and $G_t$ is the discounted return, calculated over a full episode of simulation.

While linear models and discrete action spaces (meta-actions) were initially tested, they suffered from limited expressivity and frequent policy collapse respectively. Consequently, we adopted a neural network architecture with a single hidden layer (32 units) and continuous Gaussian outputs. The network outputs means $\mu$ and standard deviations $\sigma$ for total thrust and roll, which are deterministically mapped to motor outputs. This continuous formulation better approximates the kinematic state space, removing the complexity of modelling thrust coupling, and allows $\sigma$ to be learned as an adaptive exploration parameter. Activations were selected to fit output ranges: tanh for thrust $(0,1)$ and sigmoid for roll $(-1,1)$ where saturation near extreme values is beneficial, while ReLU was used for the hidden layer to mitigate vanishing gradients.

\subsection{Stability and Normalisation}
REINFORCE is inherently high-variance. Since gradient estimates are based on stochastic Monte Carlo samples and can fluctuate wildly between episodes, convergence is unstable. To stabilise training, we employed:

\begin{itemize}
    \item \textbf{Input Normalisation:} State variables are scaled to the range $(0,1)$, so that natural heterogeneity in feature units is not propagated through to credit assignment.
    \item \textbf{Baseline:} Discounted returns $G_t$ are normalised across the episode batch (zero mean, unit variance). This acts as a baseline, preventing asymmetric reward distributions from destabilising the policy updates (e.g., with all-negative returns, even good actions are penalised)
    \item \textbf{Initialisation:} Weights are initialised using Gaussian Xavier initialisation to stabilise gradients.
\end{itemize}

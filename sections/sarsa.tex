\section{Tabular SARSA}
\paragraph{Algorithm}
Tabular methods offer superior stability and interpretability compared to deep reinforcement learning. We employ SARSA, an on-policy temporal difference algorithm that updates Q-values as: 
\begin{equation} 
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha \left[ r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) \right]
\end{equation} 
where $\alpha = 0.1$ is the learning rate and $\gamma = 0.999$ is the discount factor. Unlike off-policy Q-learning, SARSA estimates the value of the current $\epsilon$-greedy policy rather than the \textit{optimal} policy, encouraging conservative behaviour that is well-suited to hard-boundary environments. 
\paragraph{State Discretisation}
To enable tabular learning, the continuous state vector $(d_x, d_y, v_x, v_y, \theta, \omega)$ representing target-relative position, velocity, pitch, and angular velocity is discretised using a non-uniform binning strategy. Position states ($d_x, d_y$) are assigned 7 bins each, with higher resolution near the target. The vertical inner threshold is set to $|d_y| \le 0.05$ (within the target radius $R=0.10$) for high-precision altitude hold, leaving a larger error margin for the more challenging lateral control. The lateral inner threshold is set to $|d_x| \le 0.105$ (slightly outside $R$) to allow the agent to anticipate target arrival, arresting momentum to coast into the target. Dynamic states ($v_x, v_y, \theta, \omega$) use 3 bins each with low thresholds, providing high sensitivity for momentum and attitude control. Additionally, a binary \textit{danger flag} activates when the drone is within 0.1 units of the boundary. This totally yields a manageable state size of $|\mathcal{S}| = 7^2 \times 3^4 \times 2 = 7,938$.
\paragraph{Macro-Action}
Six heuristic actions map to continuous thrust pairs $(T_1,T_2)\in [0,1]$: \texttt{hover} (altitude hold with pitch stabilisation), \texttt{boost up/down} (vertical thrust modulation with level attitude), \texttt{tilt left/right} (pitch targeting to $\theta_{opt}=0.28$ rad with adaptive vertical thrust dependent on $d_y$), and \texttt{arrest motion} (damping of linear and angular velocities).
\paragraph{Reward Structure}
We balance sparse terminal signals target hit reward and crash penalty with dense shaping terms. These include a scaled progress bonus to guide navigation, a near-boundary penalty aligned with the \textit{danger flag} for immediate negative feedback, and a timestep cost to incentivise speed and path efficiency. Weights were iteratively tuned based on reward distribution logs and observed flight behaviours to prevent imbalances.
\paragraph{Curriculum Training}
Training follows three progressive stages. \textbf{Stage 0} confines targets to the safe central region, starting with full exploration. \textbf{Stage 1} expands target generation to the full environment, starting with mild exploration. To reduce control jitter and facilitate credit assignment, we enable action repetition ($n_step=3$) from this stage. \textbf{Stage 2} significantly increases the crash penalty to refine boundary-aware policies. Stage advancement is gated by rolling average target hits. Episodes terminate upon all targets acquired, boundary violation, or timeout. 

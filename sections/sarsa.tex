\section{Tabular SARSA}
Tabular methods offer superior stability and interpretability compared to deep reinforcement learning. We employ SARSA \cite{Sutton2018} , an on-policy temporal difference algorithm that updates Q-values as: 
\begin{equation} 
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha \left[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\right]
\end{equation} 
where $\alpha$ is the learning rate and $\gamma$ is the discount factor. Unlike off-policy Q-learning, SARSA estimates the value of the current $\epsilon$-greedy policy rather than the \textit{optimal} policy, encouraging conservative behaviour that is well-suited to hard-boundary environments. 
\paragraph{State Discretisation}
To enable tabular learning, the continuous state vector comprising target-relative position ($d_x, d_y$), velocity ($v_x, v_y$), pitch ($\theta$), and angular rate ($\omega$) is discretised using a non-uniform binning strategy. Target-relative positions ($d_x, d_y$) utilise a finer granularity of 7 bins with resolution increasing near the target. We define asymmetric inner thresholds where strict vertical bounds of $|d_y| \le 0.05$ ensure precise altitude hold while relaxed lateral bounds of $|d_x| \le 0.105$ (slightly outside target radius $R=0.10$) facilitate anticipatory damping. The dynamic states ($v_x, v_y, \theta, \omega$) employ coarser ternary bins with tight thresholds to maintain high sensitivity for momentum and attitude control. Additionally, a binary \textit{danger flag} activates when the drone is within 0.1 units of the boundary. This yields a tractable state space of $|\mathcal{S}| = 7,938$.
\paragraph{Macro-Action}
Six heuristic actions map to continuous thrust pairs $(T_1,T_2)\in [0,1]$: \texttt{hover} (altitude hold with pitch stabilisation), \texttt{boost up/down} (vertical thrust modulation with level attitude), \texttt{tilt left/right} (pitch targeting to $\theta_{opt}=0.28$ rad with adaptive vertical thrust dependent on $d_y$), and \texttt{arrest motion} (damping of linear and angular velocities).
\paragraph{Reward Structure}
We balance sparse terminal signals target hit reward and crash penalty with dense shaping terms. These include a scaled progress bonus to guide navigation, a near-boundary penalty aligned with the \textit{danger flag} for immediate negative feedback, and a timestep cost to incentivise speed and path efficiency. Weights were iteratively tuned based on reward distribution logs and observed flight behaviours to prevent imbalances.
\paragraph{Curriculum Training}
Training follows three progressive stages. \textbf{Stage 0} confines targets to the safe central region to facilitate initial trajectory learning. \textbf{Stage 1} expands target generation to the full environment. From this stage, we enable action repetition ($n_step=3$) to reduce control jitter and facilitate credit assignment. \textbf{Stage 2} significantly increases the crash penalty to refine boundary-aware policies. Stage advancement is gated by rolling average target acquisitions. Episodes terminate upon all targets acquired, boundary violation, or timeout. 
% Figure~\ref{fig:curriculum} illustrates the training trajectory of the \textbf{SARSA Slow} model, demonstrating clear convergence that validate the efficacy of this curriculum design.

% \begin{wrapfigure}{r}{0.4\columnwidth}
    % \centering
    % \includegraphics[width=\linewidth]{figures/Sarsa_slow_learning_curve.png}
    % \caption{Training progression of the {\textbf{SARSA Slow} model.}}
    % \label{fig:curriculum}
% \end{wrapfigure}


The control of unstable dynamical systems, such as quadcopters, presents a fundamental challenge in Reinforcement Learning (RL). Both state and action spaces are continuous, involving non-linear dynamics. A successful agent must learn both fine motor control and long-term planning. This report explores two distinct RL approaches for learning flight control and target acquisition: a discrete value-based method (SARSA) and a continuous policy-gradient method (REINFORCE). We show that REINFORCE is dominated by learning instabilities, while SARSA results in a stable and performant model.
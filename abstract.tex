The control of unstable dynamical systems, such as quadcopters, presents a fundamental challenge in Reinforcement Learning (RL). Both state and action spaces are continuous, involving non-linear dynamics. A successful agent must learn both fine motor control and long-term planning. This report explores two distinct RL approaches for learning flight control and target acquisition: a discrete value-based method (SARSA) and a continuous policy-gradient method (REINFORCE). Though the latter aligns better in theory, we are unable to overcome learning instabilities, while the former results in a stable and performant model.
The control of unstable dynamical systems, such as quadcopters, presents a fundamental challenge in Reinforcement Learning (RL). A successful agent must learn both fine motor control and navigation. We investigate the trade-off between stability and expressivity, comparing two distinct RL approaches: a discrete value-based method (SARSA) and a continuous policy-gradient method (REINFORCE). We show that REINFORCE is dominated by learning sensitivities, while SARSA results in a stable and performant model.